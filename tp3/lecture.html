<!DOCTYPE html>
<html lang="fr">
<head>
	<meta charset="utf-8">
	<title>TALN - TP 3 : Recherche d'information, Indexation.</title>
	<!-- Bootstrap -->
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<link href="../commons/css/bootstrap.min.css" rel="stylesheet" media="screen">
	<link href="../commons/css/bootstrap-responsive.css" rel="stylesheet">
	<style>
		.sidenav {
			width: 240px;
			margin: 30px 0pt 0pt;
			padding: 0pt;
			background-color: rgb(255,255,255);
			border-radius: 6px 6px 6px 6px;
			box-shadow: 0pt 1px 4px rgba(0,0,0,0.067);
		}
		.sidenav > li {
			line-height: 20px;
		}
		.sidenav > li > a {
			display: block;
			margin: 0pt 0pt -1px;
			padding: 8px 14px;
			border: 1px solid rgb(229,229,229);
		}
		.sidenav .icon-chevron-right {
			float: right;
			margin-top: 2px;
			margin-right: -6px;
			opacity: 0.25;
		}
		.footer {
			padding: 70px 0pt;
			margin-top: 70px;
			border-top: 1px solid rgb(229,229,229);
			background-color: rgb(245,245,245);
		}
	</style>
</head>
<body data-spy="scroll" data-target=".navbar">
	<div class="container-fluid">
		<h1>TALN - TP 3 : Recherche d'information, Indexation.</h1>
		<div class="row-fluid">
			<div class="span3">
				<!--Sidebar content-->
				<ul class="nav nav-list sidenav affix-top" data-spy="affix" data-offset-top="50">
					<li><a href="#intro">
						<i class="icon-chevron-right"></i>
						Fondamentaux de l'indexation
					</a></li>
					<li><a href="#brute">
						<i class="icon-chevron-right"></i>
						Recherche en texte brute
					</a></li>
					<li><a href="#equivalences">
						<i class="icon-chevron-right"></i>
						Classes d'équivalences
					</a></li>
					<li><a href="#ponderation">
						<i class="icon-chevron-right"></i>
						Pondérations
					</a></li>
					<li><a href="#application">
						<i class="icon-chevron-right"></i>
						Mise en application
					</a></li>
				</ul>
			</div>
			<div class="span7">

				<!-- SYNOPSIS -->
<p class="lead">L'objectif de cette séance est d'intégrer les principes fondamentaux de l'indexation de documents pour la <abbr title="Recherche d'Information">RI</abbr> : choix des mots à indexer et classes d'équivalence.</p>

				<!-- SECTION 1 - FONDAMENTAUX -->
<a name="intro"></a>
<h2>Fondamentaux de l'indexation</h2>

<p>En <abbr title="Recherche d'Information">RI</abbr>, la phase d'indexation est l'étape qui consiste à enregistrer un document dans le système de sorte qu'il puisse être retourné à l'utilisateur s'il est pertinent par rapport à sa requête.</p>

<p>Wikipedia distingue deux types d'indexation :</p>
<blockquote>
<p>L’<em class="text-info">indexation d'un texte</em> consiste à repérer dans celui-ci certains mots ou expressions particulièrement significatifs (appelés termes) dans un contexte donné, afin de créer un index terminologique.</p>
<p>L’<em class="text-info">indexation automatique de documents</em> est un ensemble de techniques au croisement de l'informatique et des Sciences de l'information et des bibliothèques qui permettent de repérer des éléments significatifs dans des documents numériques comme des documents textuels, des pages web, etc. Elle est notamment utilisée par les moteurs de recherche.</p>
<small><a href="http://fr.wikipedia.org/wiki/Indexation">Wikipedia</a></small>
</blockquote>

<p>La deuxième définition est celle à laquelle nous nous intéressons. Nous reviendrons toutefois plus tard sur la première qui est extrêmement pertinente pour certains types de documents écrits dans des langues dites de <em>spécialités</em> (documentations techniques, textes de lois, ...).</p>

<p>Plus prosaïquement, l'indexation d'un document revient à introduire ledit document dans un index (ou à créer cet index s'il n'existe pas). Un index est une liste de descripteurs (des mots ou des expressions dans notre cas) auxquels sont associés des documents qui renvoient vers ledit descripteur. L'index est en général une structure de données de type <em class="text-info">index inversé</em> qui associe à du contenu sa position.</p>

				<!-- SECTION 2 - TEXTE BRUTE -->
<a name="brute"></a>
<h2>Recherche en texte brute</h2>

<p>Une première indexation quelque peu naïve consisterait à construire un index inversé des mots tels qu'ils apparaîssent dans le texte, à la façon du listing suivant :</p>

<pre class="prettyprint linenum">
"art"        {D1, D2}
"plaire"     {D1}
"est"        {D1}
"tromper"    {D1}
"nous"       {D2}
"avons"      {D2}
"afin"       {D2}
"pas"        {D2}
"mourir"     {D2}
"vérité"     {D2}
</pre>

<p>La structure de données <a href="http://docs.python.org/2/tutorial/datastructures.html#dictionaries">dictionnaire de Python</a> est parfaitement adapté à l'exercice. De plus, lors des séances précédents vous avez mis au point un tokeniseur en mots que vous pouvez directement réutiliser ici.</p>

<p class="text-warning"><strong>Écrivez un script python permettant de construire un index inversé des mots de fichiers textes passés en paramètre puis faites le tourner sur le corpus <code>wikinews</code>. Réalisez quelques requêtes sur l'index.</strong></p>

				<!-- SECTION 3 - CLASSES D'ÉQUIVALENCE -->
<a name="equivalences"></a>
<h2>Classes d'équivalences</h2>

<p>Un rapide coup d'œil sur l'index construit à la section précédente montre qu'un certain nombre d'entrées mériteraient d'être regroupées. C'est le cas par exemple pour les mots : <em>apportait, apporte, apporter, apporterait, apportée</em>. Intuitivement, ces mots semblent sémantiquement équivalents : ils portent plus ou moins le même sens et il y a forte à parier qu'un utilisateur qui rechercherait un document avec <em>apporte</em> serait tout aussi intéressé par un autre document qui contiendrait le mot <em>apporté</em>.</p>

<p>Les classes d'équivalences servent ce besoin. Leur principe est simple : regrouper sous une même entrée plusieurs entrées de l'index qui sont sémantiquement équivalentes. Bien sûr les faits sont bien plus ingrâts, il est extrêmement difficile de savoir si des mots sont sémantiquement équivalents au sein d'un texte et d'autant plus au sein de différents documents.</p>

<p>Une approximation courante de l'équivalence sémantique est de construire des classes d'équivalence regroupant des mots ayant la même racine. L'idée est que la racine porte un sens général commun à tous les mots qui en dérivent.</p>

<h3>Un peu de racinisation</h3>

<p>Comme vous l'avez vu en cours, l'algorithme de Porter permet de retrouver la racine d'un mot en appliquant itérativement diverses règles morphologiques basées sur les suffixes.</p>

<p><abbr title="Natural Language ToolKit">NLTK</abbr> propose une implémentation de l'algorithme de Porter que vous pouvez retrouver sur <a href="http://nltk.org/_modules/nltk/stem/porter.html">cette page</a>. Allez jeter un œil à cette page et observez comment l'algorithme a été implémenté.</p>

<p>L'algorithme de Porter a été originellement développé pour l'anglais. Il existe toutefois une implémentation multilingue de racinisation basée sur les principes de Porter : <code>snowball</code>. <abbr title="Natural Language ToolKit">NLTK</abbr> en propose <a href="http://nltk.org/_modules/nltk/stem/snowball.html#FrenchStemmer">une implémentation pour le français</a>.</p>

<p class="text-warning"><strong>Installez <abbr title="Natural Language ToolKit">NLTK</abbr> localement sur votre machine en suivant <a href="http://nltk.org/install.html">ces indications</a>.</strong></p>

<div class="accordion" id="note-ntlk-setup">
  <div class="accordion-group">
    <div class="accordion-heading">
      <a class="accordion-toggle" data-toggle="collapse" data-parent="#note-nltk-setup" href="#collapseOne">
        <i class="icon-question-sign"> </i> Détails sur l'installation de <abbr title="Natural Language ToolKit">NLTK</abbr>
      </a>
    </div>
    <div id="collapseOne" class="accordion-body collapse">
      <div class="accordion-inner">
        <p>Il se peut que l'installation soit rendue compliquée voir impossible par l'absence de certains modules. Pour faire court, il reste envisageable d'installer localement les archives.</p>
        <p>Téléchargez l'archive contenant la distribution <abbr title="Natural Language ToolKit">NLTK</abbr> dans un répertoire dédié.</p>
        <pre class="prettyprint linenum">
$ mkdir ~/python-libs
$ cd ~/python-libs
$ wget http://pypi.python.org/packages/source/n/nltk/nltk-2.0.4.tar.gz
--2012-12-02 23:25:11--  http://pypi.python.org/packages/source/n/nltk/nltk-2.0.4.tar.gz
Résolution de pypi.python.org (pypi.python.org)... 140.211.10.73
Connexion vers pypi.python.org (pypi.python.org)|140.211.10.73|:80...connecté.
requête HTTP transmise, en attente de la réponse...200 OK
Longueur: 955978 (934K) [application/octet-stream]
Sauvegarde en : «nltk-2.0.4.tar.gz»

100%[======================================>] 955 978     21,0K/s   ds 39s     

2012-12-02 23:25:50 (24,1 KB/s) - «nltk-2.0.4.tar.gz» sauvegardé [955978/955978]
        </pre>
        <p>Extrayez le contenu de l'archive et créez un lien symbolique vers le module <code>nltk-2.0.4/nltk</code>.</p>
        <pre class="prettyprint linenum">
$ tar -xzvf nltk-2.0.4.tar.gz
...
nltk-2.0.4/javasrc/org/nltk/mallet/CRFInfo.java
nltk-2.0.4/javasrc/org/nltk/mallet/RunCRF.java
nltk-2.0.4/javasrc/org/nltk/mallet/TrainCRF.java
$ ln -s nltk-2.0.4/nltk .
        </pre>
        <p>Déclarez la variable d'environnement <code>PYTHONPATH</code> et faites la pointer vers le répertoire contenant le module.</p>
        <pre class="prettyprint linenum">
$ export PYTHONPATH=$PYTHONPATH:~/python-libs/
        </pre>
        <p><i class="icon-warning-sign"> </i>Si vous avez une erreur concernant une variable non liée, supprimez simplement la référence à <code>$PYTHONPATH</code>.</p>
        <pre class="prettyprint linenum">
$ export PYTHONPATH=~/python-libs/
        </pre>
        <p>Vérifiez que l'installation fonctionne en important le module dans une console python.</p>
        <pre class="prettyprint linenum">
$ python
Python 2.7.3rc2 (default, Apr 22 2012, 22:30:17) 
[GCC 4.6.3] on linux2
Type "help", "copyright", "credits" or "license" for more information.
>>> import nltk
>>>
        </pre>
        <p>Afin de conserver cette configuration après un redémarrage, ajoutez la ligne <code>export PYTHONPATH=$PYTHONPATH:~/python-libs/</code> dans votre fichier <code>~/.bashrc</code>.</p>
      </div>
    </div>
  </div>
  <div class="accordion-group">
    <div class="accordion-heading">
      <a class="accordion-toggle" data-toggle="collapse" data-parent="#note-nltk-setup" href="#collapseTwo">
        <i class="icon-question-sign"> </i> Détails sur l'installation de <abbr title="Python for Yet another Markup Language">PyYaml</abbr>
      </a>
    </div>
    <div id="collapseTwo" class="accordion-body collapse">
      <div class="accordion-inner">
        <p><abbr title="Natural Language ToolKit">NLTK</abbr> dépend d'un autre module Python : <a href="http://pyyaml.org/">PyYaml</a>. Vous pouvez l'installer en suivant les mêmes instructions que précédemment.</p>
      <pre class="prettyprint linenum">
$ cd ~/python-libs/
$ wget http://pyyaml.org/download/pyyaml/PyYAML-3.10.tar.gz
...
$ tar -xzvf PyYAML-3.10.tar.gz
...
$ ln -s PyYAML-3.10/lib/yaml .
      </pre>
      </div>
    </div>
  </div>
</div>

<p>Une fois le module <abbr title="Natural Language ToolKit">NLTK</abbr> installé, expérimentez le racinisateur pour le français. Évaluez sa pertinence pour différents mots du corpus.</p>

<pre class="prettyprint linenum">
>>> from nltk.stem.snowball import FrenchStemmer
>>> stemmer = FrenchStemmer()
>>> help(stemmer)
>>> stemmer.stem("racinisation")
u'racinis'
>>> stemmer.stem("engouement")
u'engou'
>>> stemmer.stem(u"étais")
u'\xe9tais'
>>> stemmer.stem(u"Premièrement")
u'premi'
</pre>

<h3>Indexation</h3>

<p>La racinisation lors de la phase d'indexation peut être exclusive ou non : soit l'on indexe uniquement les racines des mots, soit l'on indexe les racines et les formes rencontrées dans le texte.</p>

<p class="text-warning"><strong>Reprenez votre script d'indexation écrit précédemment et modifiez-le pour tester l'impact de la racinisation. Profitez-en pour filtrer automatiquement les mots outils qui n'ont pas d'intérêt pour votre index.</strong></p>

				<!-- SECTION 4 - PONDÉRATIONS -->
<a name="ponderation"></a>
<h2>Pondérations</h2>

<p>Si vous indexez un grand nombre de documents, vos requêtes raméneront mécaniquement un grand nombre de documents. Il est nécessaire de pouvoir ordonner les documents retournés selon leur pertinence, de sorte à ne proposer à l'utilisateur que les 10 plus pertinents par exemple.</p>

<p>Une première forme de pondération pourrait s'effectuer par requête : le système donnerait le score le plus élevé aux documents qui contiennent le plus de mots de la requête.</p>

<p class="text-warning">Modifiez la fonction <code>query()</code> pour donner un score aux documents basé sur le nombre de mots de la requête qu'ils contiennent.</p>

<p>Malheureusement l'approche précédente n'améliorera que marginalement les résultats car d'expérience les utilisateurs soumettent des requêtes contenant un nombre limité de mots clés. Il est donc nécessaire de trouver un autre moyen d'ordonner les résultats.</p>

<p><a href="http://en.wikipedia.org/wiki/Gerard_Salton">G. Salton</a>, un des fondateurs de la <abbr title="Recherche d'Information">RI</abbr> moderne a notamment introduit la mesure de <abbr title="Term Frequency per Inverse Document Frenquency">tf.idf</abbr>.</p>

<blockquote>
<p>Le TF-IDF est une méthode de pondération souvent utilisée en recherche d'information et en particulier dans la fouille de textes. Cette mesure statistique permet d'évaluer l'importance d'un terme contenu dans un document, relativement à une collection ou un corpus.</p>
<a href="http://fr.wikipedia.org/wiki/TF-IDF">Wikipedia</a>
</blockquote>

<p>Le calcul du <abbr title="Term Frequency per Inverse Document Frenquency">tf.idf</abbr> nécessite de mener l'indexation en deux temps. Dans un premier temps, les mots sont comptés et ajoutés dans l'index tandis que l'on calcule leur <em>document frequency</em>. Dans un second temps, le score de chaque mot pour chaque document est mis à jour en multipliant la fréquence du mot dans le document par le logarithme de l'inverse du <em>document frequency</em> du mot.</p>

<p>Voici une proposition de framework pour ce calcul en deux temps :</p>

<pre class="prettyprint linenum">
index = {}
df = {}
nbdocs = 0
# première passe
for doc in documents:
  nbdocs += 1
  localindex = {}
  nbwords = 0
  for word in tokenize(doc):
    nbwords += 1
    # calcule du nombre d'occurrences
    if not localindex.has_key(word):
      localindex[word] = 1
      # maj du df
      if not df.has_key(word):
        df[word] = 1
      else:
        df[word] += 1
    else:
      index[word] += 1
  # maj de l'index avec le tf
  for w,occ in localindex.items():
    if not index.has_key(w):
      index[w] = []
    index.append( (doc, 1.*occ/nbwords) )
# seconde passe
for w,entries in index.items():
  for i in range(len(entries)):
    doc,tf = entries[i]
    entries[i] = (doc, tf, tf * math.log(1. * nbdocs / df[w]))
</pre>

<p class="text-warning"><strong>Appropriez-vous ce code puis modifiez votre script d'indexation et vos fonctions de requêtage pour tenir compte de la pondération <abbr title="Term Frequency per Inverse Document Frenquency">tf.idf</abbr>.</strong></p>

				<!-- SECTION 5 - APPLICATION -->
<a name="application"></a>
<h2>Mise en application</h2>

<p class="text-warning"><strong>Indexez la totalité du corpus Wikinews à votre disposition et tentez quelques recherches dans votre index.</strong></p>

			</div>
		</div>
	</div>
	<footer class="footer">
		<div class="container">
			<div class="span8">
				<dl>
					<dt>Corpus, code, ...</dt>
					<dd><a href="https://github.com/grdscarabe/nlp-lessons">https://github.com/grdscarabe/nlp-lessons</a></dd>
					<dt>Lectures</dt>
					<dd><a href="http://www.fabienpoulard.info">Blog de l'auteur</a></dd>
				</dl>
			</div>
			<div class="span3">
				<address>
					<strong>Fabien Poulard</strong><br>
					Dictanova SAS<br>
					2, chemin de la Houssinière<br/>
					44300 Nantes</br>
					<a href="mailto:fpoulard@dictanova.com">fpoulard@dictanova.com</a>
				</address>
			</div>
		</div>
	</footer>
	<!-- JS -->
	<script src="http://code.jquery.com/jquery-latest.js"></script>
	<script src="../commons/js/bootstrap.min.js"></script>
</body>
</html>


