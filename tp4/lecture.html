<!DOCTYPE html>
<html lang="fr">
<head>
	<meta charset="utf-8">
	<title>TALN - TP 4 : Recherche d'information, Indexation avancée.</title>
	<!-- Bootstrap -->
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<link href="../commons/css/bootstrap.min.css" rel="stylesheet" media="screen">
	<link href="../commons/css/bootstrap-responsive.css" rel="stylesheet">
	<style>
		.sidenav {
			width: 240px;
			margin: 30px 0pt 0pt;
			padding: 0pt;
			background-color: rgb(255,255,255);
			border-radius: 6px 6px 6px 6px;
			box-shadow: 0pt 1px 4px rgba(0,0,0,0.067);
		}
		.sidenav > li {
			line-height: 20px;
		}
		.sidenav > li > a {
			display: block;
			margin: 0pt 0pt -1px;
			padding: 8px 14px;
			border: 1px solid rgb(229,229,229);
		}
		.sidenav .icon-chevron-right {
			float: right;
			margin-top: 2px;
			margin-right: -6px;
			opacity: 0.25;
		}
		.footer {
			padding: 70px 0pt;
			margin-top: 70px;
			border-top: 1px solid rgb(229,229,229);
			background-color: rgb(245,245,245);
		}
	</style>
</head>
<body data-spy="scroll" data-target=".navbar">
	<div class="container-fluid">
		<h1>TALN - TP 4 : Recherche d'information, Indexation avancée.</h1>
		<div class="row-fluid">
			<div class="span3">
				<!--Sidebar content-->
				<ul class="nav nav-list sidenav affix-top" data-spy="affix" data-offset-top="50">
					<li><a href="#postagging">
						<i class="icon-chevron-right"></i>
						Étiquetage des rôles grammaticaux
					</a></li>
					<li><a href="#substantifs">
						<i class="icon-chevron-right"></i>
						Indexation des substantifs
					</a></li>
					<li><a href="#terms">
						<i class="icon-chevron-right"></i>
						Indexation de termes
					</a></li>
					<li><a href="#keyphrases">
						<i class="icon-chevron-right"></i>
						Expressions clés
					</a></li>
					<li><a href="#application">
						<i class="icon-chevron-right"></i>
						Mise en application
					</a></li>
				</ul>
			</div>
			<div class="span7">

				<!-- SYNOPSIS -->
<p class="lead">L'objectif de cette séance est d'aller au-delà de l'indexation lexicale naïve pour se concentrer sur les éléments qui expriment le sens des textes.</p>

				<!-- SECTION 1 - POS TAGGING -->
<a name="postagging"></a>
<h2>Étiquetage des rôles grammaticaux</h2>

<h3>Intuition</h3>

<p>Un étiqueteur morpho-syntaxique est un outil permettant d’assigner automatiquement à chacun des mots (au sens tokens) d’un texte, sa catégorie grammaticale (e.g. déterminant, adjectif, nom). Un exemple de phrase tokenisée et étiquetée en catégories morpho-syntaxiques est présentée ci-dessous :</p>

<pre class="prettyprint">
“J’ adore les sushis .” → “J’/CL adore/V les/D sushis/N ./.”
</pre>

<p>L'intuition mise en œuvre par les systèmes d'étiquetage des rôles grammaticaux est que les mots de même rôle partagent des contextes similaires. En d'autres termes, dis-moi quels sont tes voisins, je te dirai qui tu es !</p>

<p class="text-warning"><strong>Écrire un script permettant d'extraire les contextes d'un mot donné, puis de rechercher les mots ayant un contexte similaire. Le corpus wikinews ne sera pas suffisant</strong></p>

<h3>Annotation de corpus</h3>

<p>La bibliothèque nltk contient plusieurs méthodes d’étiquetage morpho-syntaxique mais elle ne dispose que de modèles pour la langue anglaise. Il va donc nous falloir construire une ressource pour le français...</p>

<p>L'une des solutions consisterait à vous faire annoter les rôles grammaticaux sur un corpus en français tel que le wikinews. Ce type d'annotation est cependant assez technique et relève plutôt du travail des linguistes.</p>

<p>Nous allons utiliser le corpus <em><a href="https://gforge.inria.fr/frs/download.php/31734/sequoia-corpus-v4.0.tgz">séquoia</a></em> qui est un corpus arboré pour le français. Il va cependant être nécessaire dans un premier temps de transformer l'annotation arborée en un étiquetage <code>mot/pos</code>.</p>

<pre class="prettyprint">
line = u"( (SENT (NP-SUJ (DET Ce) (NC document)) (VN (V est)) (NP-ATS (DET un) (NC résumé) (PP (P+D du) (NP (NC rapport) (AP (ADJ européen)) (AP (ADJ public)) (PP (P d') (NP (NC évaluation))) (PONCT -LRB-) (NP (NC EPAR)) (PONCT -RRB-)))) (PONCT .)))"
from nltk.tree import Tree
sent = Tree.parse(line)
tokens = [u"%s/%s"%(t,p) for t,p in sent.pos() if not t in ["-LRB-", "-RRB-"]]
u" ".join(tokens)
</pre>

<h3>Entraînement d'un étiqueteur à unigrammes</h3>

<p>Le principe d'un étiqueteur à unigrammes est d'associer à un mot sa catégorie grammaticale la plus probable. Un tel étiqueteur est extrêmement naïf et n'utilise nullement le contexte du mot.</p>

<p>NLTK propose une implémentation d'étiqueteur à unigrammes : <code>nltk.UnigramTagger</code>. Celui-ci doit être entraîné à partir d'une liste de phrases dont les mots sont étiquetés. Chaque phrase est représentée comme une liste de tuples <code>(mot, étiquette)</code>. La fonction <code>nltk.tag.str2tuple</code> permet de convertir nos annotations <code>mot/étiquette</code> en de tels tuples.</p>

<pre class="prettyprint">
import nltk
sentstr = u"Ce/DET document/NC est/V un/DET résumé/NC du/P+D rapport/NC européen/ADJ public/ADJ d'/P évaluation/NC EPAR/NC ./PONCT"
senttuple = [nltk.tag.str2tuple(tok) for tok in sentstr.split()]
tagger = nltk.UnigramTagger([senttuple])
tagger.tag(u"Ce document est un résumé du rapport européen public d' évaluation EPAR .".split())
</pre>

<p class="text-warning"><strong>Entraînez un étiqueteur à unigrammes sur le corpus séquoia. Testez-le sur quelques phrases.</strong></p>

<h3>Évaluation d'un étiqueteur à unigrammes</h3>

<p class="text-warning"><strong>Étudiez le script <code>train-unigram-on-sequoia.py</code>, puis utilisez-le pour évaluer la qualité de l'étiqueteur à unigrammes sur le corpus séquoia.</strong></p>

				<!-- SECTION 2 - SUBSTANTIFS -->
<a name="substantifs"></a>
<h2>Indexation des substantifs</h2>

				<!-- SECTION 3 - TERMS -->
<a name="terms"></a>
<h2>Indexation de termes</h2>

<h3>Motifs terminologiques</h3>

<h3>Normalisation des termes</h3>

<h3>Indexation</h3>

				<!-- SECTION 4 - KEY PHRASES -->
<a name="keyphrases"></a>
<h2>Expressions clés</h2>

<p>Les keyphrases, ou termes clés en français, sont des termes permettant de caractériser le contenu d’un document. Considéré comme un bref résumé, ils permettent d’organiser et de retrouver plus facilement les documents dans une collection. De nombreuses applications en TALN peuvent également bénéficier de la présence de keyphrases, e.g. le résumé automatique, l’indexation ou les systèmes de questions-réponses. Cependant, très peu de documents ont un ensemble de keyphrases associé et l’annotation manuelle de grandes collections serait bien trop coûteuse. La problématique de l’extraction automatique keyphrases a donc naturellement émergée et de nombreux travaux ont été publiés à ce sujet.</p

<p>Les approches proposées par la communauté peuvent être réparties en deux catégories : supervisées et non-supervisées. Les approches supervisées considèrent l’extraction de keyphrases comme une problème de classification binaire (keyphrase ou pas keyphrase). Ce type d’approche est connu pour donner les meilleurs résultats mais nécessite un ensemble de données annotées pour l’entraînement. De plus, les performances des modèles appris sont dépendantes de la qualité de l’annotation, de la quantité de données et du domaine traité.</p>

<p>Pour éviter ses problèmes, de nombreuses méthodes non-supervisées ont été proposées. Ces dernières utilisent divers critères statistiques et/ou linguistiques pour évaluer l’importance d’un terme vis-à-vis du document, e.g. tf × idf, TextRank, information mutuelle, distance dans une ontologie.</p>

<p>Les différentes étapes d’une méthode baseline pour l’extraction de keyphrases sont présentées ci-dessous :</p>
<ol>
	<li>Pré-traitement du document
	<ul>
		<li>Découpage du texte en phrases</li>
		<li>Tokenization</li>
		<li>Étiquetage morphosyntaxique</li>
	</ul>
	</li>
	<li>Pondération des mots
	<ul>
		<li>Calcul du scores des mots, e.g. tf × idf</li>
	</ul>
	</li>
	<li>Génération des termes candidats
	<ul>
		<li>Utilisation de patrons syntaxiques, e.g. (Adj∗)(NPP|NC)+</li>
	</ul>
	</li>
	<li>Pondération et ordonnancement des termes candidats
	<ul>
		<li>Calcul du score des termes, e.g. mot∈terme score(mot)</li>
		<li>Tri des termes par ordre décroissant de score</li>
	</ul>
	</li>
	<li>Génération des keyphrases et évaluation
	<ul>
		<li>Sélection des n-meilleurs termes candidats</li>
	</ul>
</ol>

<h3>Approche à base de n-grammes</h3>

<h3>Approche terminologique</h3>


				<!-- SECTION 5 - APPLICATION -->
<a name="application"></a>
<h2>Mise en application</h2>

<p class="text-warning"><strong>Indexez la totalité du corpus Wikinews à votre disposition et tentez quelques recherches dans votre index.</strong></p>

			</div>
		</div>
	</div>
	<footer class="footer">
		<div class="container">
			<div class="span8">
				<dl>
					<dt>Ressources</dt>
					<dd>
						<ul>
							<li><a href="http://nltk.googlecode.com/svn/trunk/doc/book/ch05.html">Chapitre du livre NLTK sur l'étiquetage</a></li>
							<li>Billet de streamhacker sur le <a href="http://streamhacker.com/2011/03/21/training-part-speech-taggers-nltk-trainer/">pos tagging</a> et le <a href="http://streamhacker.com/2008/12/29/how-to-train-a-nltk-chunker/">chunking</a></li>
							<li><a href="https://gforge.inria.fr/frs/download.php/31734/sequoia-corpus-v4.0.tgz">Séquoia : un corpus arboré pour le français<a/></li>
						</ul>
					</dd>
					<dt>Corpus, code, ...</dt>
					<dd><a href="https://github.com/grdscarabe/nlp-lessons">https://github.com/grdscarabe/nlp-lessons</a></dd>
					<dt>Lectures</dt>
					<dd><a href="http://www.fabienpoulard.info">Blog de l'auteur</a></dd>
				</dl>
			</div>
			<div class="span3">
				<address>
					<strong>Fabien Poulard</strong><br>
					Dictanova SAS<br>
					2, chemin de la Houssinière<br/>
					44300 Nantes</br>
					<a href="mailto:fpoulard@dictanova.com">fpoulard@dictanova.com</a>
				</address>
			</div>
		</div>
	</footer>
	<!-- JS -->
	<script src="http://code.jquery.com/jquery-latest.js"></script>
	<script src="../commons/js/bootstrap.min.js"></script>
</body>
</html>


