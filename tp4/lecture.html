<!DOCTYPE html>
<html lang="fr">
<head>
	<meta charset="utf-8">
	<title>TALN - TP 4 : Recherche d'information, Indexation avancée.</title>
	<!-- Bootstrap -->
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<link href="../commons/css/bootstrap.min.css" rel="stylesheet" media="screen">
	<link href="../commons/css/bootstrap-responsive.css" rel="stylesheet">
	<style>
		.sidenav {
			width: 240px;
			margin: 30px 0pt 0pt;
			padding: 0pt;
			background-color: rgb(255,255,255);
			border-radius: 6px 6px 6px 6px;
			box-shadow: 0pt 1px 4px rgba(0,0,0,0.067);
		}
		.sidenav > li {
			line-height: 20px;
		}
		.sidenav > li > a {
			display: block;
			margin: 0pt 0pt -1px;
			padding: 8px 14px;
			border: 1px solid rgb(229,229,229);
		}
		.sidenav .icon-chevron-right {
			float: right;
			margin-top: 2px;
			margin-right: -6px;
			opacity: 0.25;
		}
		.footer {
			padding: 70px 0pt;
			margin-top: 70px;
			border-top: 1px solid rgb(229,229,229);
			background-color: rgb(245,245,245);
		}
	</style>
</head>
<body data-spy="scroll" data-target=".navbar">
	<div class="container-fluid">
		<h1>TALN - TP 4 : Recherche d'information, Indexation avancée.</h1>
		<div class="row-fluid">
			<div class="span3">
				<!--Sidebar content-->
				<ul class="nav nav-list sidenav affix-top" data-spy="affix" data-offset-top="50">
					<li><a href="#postagging">
						<i class="icon-chevron-right"></i>
						Étiquetage des rôles grammaticaux
					</a></li>
					<li><a href="#terms">
						<i class="icon-chevron-right"></i>
						Indexation des termes clés
					</a></li>
					<li><a href="#application">
						<i class="icon-chevron-right"></i>
						Mise en application
					</a></li>
				</ul>
			</div>
			<div class="span7">

				<!-- SYNOPSIS -->
<p class="lead">L'objectif de cette séance est d'aller au-delà de l'indexation lexicale naïve pour se concentrer sur les éléments qui expriment le sens des textes.</p>

				<!-- SECTION 1 - POS TAGGING -->
<a name="postagging"></a>
<h2>Étiquetage des rôles grammaticaux</h2>

<h3>Intuition</h3>

<p>Un étiqueteur morpho-syntaxique est un outil permettant d’assigner automatiquement à chacun des mots (au sens tokens) d’un texte, sa catégorie grammaticale (e.g. déterminant, adjectif, nom). Un exemple de phrase tokenisée et étiquetée en catégories morpho-syntaxiques est présentée ci-dessous :</p>

<pre class="prettyprint">
“J’ adore les sushis .” → “J’/CL adore/V les/D sushis/N ./.”
</pre>

<p>L'intuition mise en œuvre par les systèmes d'étiquetage des rôles grammaticaux est que les mots de même rôle partagent des contextes similaires. En d'autres termes, dis-moi quels sont tes voisins, je te dirai qui tu es !</p>

<p class="text-warning"><strong>Écrire un script permettant d'extraire les contextes d'un mot donné, puis de rechercher les mots ayant un contexte similaire. Le corpus wikinews ne sera pas suffisant</strong></p>

<h3>Annotation de corpus</h3>

<p>La bibliothèque nltk contient plusieurs méthodes d’étiquetage morpho-syntaxique mais elle ne dispose que de modèles pour la langue anglaise. Il va donc nous falloir construire une ressource pour le français...</p>

<p>L'une des solutions consisterait à vous faire annoter les rôles grammaticaux sur un corpus en français tel que le wikinews. Ce type d'annotation est cependant assez technique et relève plutôt du travail des linguistes.</p>

<p>Nous allons utiliser le corpus <em><a href="https://gforge.inria.fr/frs/download.php/31734/sequoia-corpus-v4.0.tgz">séquoia</a></em> qui est un corpus arboré pour le français. Il va cependant être nécessaire dans un premier temps de transformer l'annotation arborée en un étiquetage <code>mot/pos</code>.</p>

<pre class="prettyprint">
line = u"( (SENT (NP-SUJ (DET Ce) (NC document)) (VN (V est)) (NP-ATS (DET un) (NC résumé) (PP (P+D du) (NP (NC rapport) (AP (ADJ européen)) (AP (ADJ public)) (PP (P d') (NP (NC évaluation))) (PONCT -LRB-) (NP (NC EPAR)) (PONCT -RRB-)))) (PONCT .)))"
from nltk.tree import Tree
sent = Tree.parse(line)
tokens = [u"%s/%s"%(t,p) for t,p in sent.pos() if not t in ["-LRB-", "-RRB-"]]
u" ".join(tokens)
</pre>

<h3>Entraînement d'un étiqueteur à unigrammes</h3>

<p>Le principe d'un étiqueteur à unigrammes est d'associer à un mot sa catégorie grammaticale la plus probable. Un tel étiqueteur est extrêmement naïf et n'utilise nullement le contexte du mot.</p>

<p>NLTK propose une implémentation d'étiqueteur à unigrammes : <code>nltk.UnigramTagger</code>. Celui-ci doit être entraîné à partir d'une liste de phrases dont les mots sont étiquetés. Chaque phrase est représentée comme une liste de tuples <code>(mot, étiquette)</code>. La fonction <code>nltk.tag.str2tuple</code> permet de convertir nos annotations <code>mot/étiquette</code> en de tels tuples.</p>

<pre class="prettyprint">
import nltk
sentstr = u"Ce/DET document/NC est/V un/DET résumé/NC du/P+D rapport/NC européen/ADJ public/ADJ d'/P évaluation/NC EPAR/NC ./PONCT"
senttuple = [nltk.tag.str2tuple(tok) for tok in sentstr.split()]
tagger = nltk.UnigramTagger([senttuple])
tagger.tag(u"Ce document est un résumé du rapport européen public d' évaluation EPAR .".split())
</pre>

<p class="text-warning"><strong>Entraînez un étiqueteur à unigrammes sur le corpus séquoia. Testez-le sur quelques phrases.</strong></p>

<h3>Évaluation d'un étiqueteur à unigrammes</h3>

<p class="text-warning"><strong>Étudiez le script <code>train-unigram-on-sequoia.py</code>, puis utilisez-le pour évaluer la qualité de l'étiqueteur à unigrammes sur le corpus séquoia.</strong></p>

<h3>Entraînement d'un étiqueteur de Brill</h3>

<p>L'entraînement d'un étiqueteur de Brill est extrêmement similaire dans le principe à celui de l'étiqueteur par unigrammes : utiliser des phrases déjà étiquetées manuellement pour construire le modèle. Cependant, le processus est un peu plus compliqué puisque l'étiqueteur de Brill travaille en deux phases : un premier étiquetage hors contexte puis la correction des étiquettes sur la base de règles contextuelles. L'apprentissage revient finalement à apprendre ces règles de correction.</p>

<p>Dans NLTK, l'entraînement de l'étiqueteur de Brill nécessite :</p>
<ul>
	<li>un étiqueteur de référence pour la première passe : nous utiliserons l'étiqueteur par unigrammes.</li>
	<li>des modèles de règles à apprendre basées sur l'exploitation du contexte.</li>
</ul>

<pre class="prettyprint">
templates = [
	# Context tag in a 1, 2 and 3 word window
	SymmetricProximateTokensTemplate(ProximateTagsRule, (1,1)),
	SymmetricProximateTokensTemplate(ProximateTagsRule, (2,2)),
	SymmetricProximateTokensTemplate(ProximateTagsRule, (1,2)),
	SymmetricProximateTokensTemplate(ProximateTagsRule, (1,3)),
	# Context word in a 1, 2 and 3 word window
	SymmetricProximateTokensTemplate(ProximateWordsRule, (1,1)),
	SymmetricProximateTokensTemplate(ProximateWordsRule, (2,2)),
	SymmetricProximateTokensTemplate(ProximateWordsRule, (1,2)),
	SymmetricProximateTokensTemplate(ProximateWordsRule, (1,3)),
	# Closest tag
	ProximateTokensTemplate(ProximateTagsRule, (-1, -1), (1,1)),
	# Closest word
	ProximateTokensTemplate(ProximateWordsRule, (-1, -1), (1,1))
]
</pre>

<p class="text-warning"><strong>Étudiez le script <code>train-brill-on-sequoia.py</code>, puis utilisez-le pour évaluer la qualité de l'étiqueteur de Brill sur le corpus séquoia.</strong></p>

<p class="text-warning"><strong>Observez et comprenez les règles de correction retenues.</strong></p>

				<!-- SECTION 3 - KEY PHRASES -->
<a name="terms"></a>
<h2>Indexation des termes clés</h2>

<p>Les termes clés, ou <em>keyphrases</em>, sont des termes permettant de caractériser le contenu d’un document. Considéré comme un bref résumé, ils permettent d’organiser et de retrouver plus facilement les documents dans une collection. Ils sont extrêmement pertinents pour l'indexation car ils portent le sens du document.</p>

<p>Les approches proposées par la communauté peuvent être réparties en deux catégories : supervisées et non-supervisées. Les approches supervisées donnent de meilleurs résultats mais nécessitent un ensemble de données annotées pour l’entraînement. Les méthodes non-supervisées utilisent divers critères statistiques et/ou linguistiques pour évaluer l’importance d’un terme vis-à-vis du document : tf × idf, TextRank, information mutuelle, distance dans une ontologie...</p>

<p>Les différentes étapes d’une méthode baseline pour l’extraction de keyphrases sont :</p>
<ol>
	<li>Pré-traitement du document : découpage en phrases, en mots et étiquetage des rôles grammaticaux.</li>
	<li>Pondération des mots (tf × idf)</li>
	<li>Génération des termes candidats à l'aide de patrons syntaxiques</li>
	<li>Pondération et ordonnancement des termes candidats</li>
	<li>Sélection des n-meilleurs termes candidats</li>
</ol>

<h3>Génération des termes candidats</h3>

<p>Précédemment, nous considérions tous les mots rencontrés comme pertinents. Cette approche relève plus du désespoir qu'autre chose. Maintenant que nous avons à disposition des outils plus pointus, nous pouvons être un peu plus sélectifs.</p>

<p>Nous allons désormais nous concentrer sur :</p>
<ul>
	<li>Les noms et les verbes</li>
	<li>Les motifs (Adj∗)(NPP|NC)+ et (NPP|NC)+(de|à)(NPP|NC)+</li>
</ul>

<p class="text-warning"><strong>Utilisez les briques mises au point précédemment pour extraire les termes candidats d'un texte.</strong></p>

<h3>Pondération des termes candidats</h3>

<p class="text-warning"><strong>Calculez le tf × idf des différents termes candidats extraits précédemment. Incorporez ce score à l'indexation.</strong></p>

				<!-- SECTION 5 - APPLICATION -->
<a name="application"></a>
<h2>Mise en application</h2>

<p class="text-warning"><strong>Indexez la totalité du corpus Wikinews à votre disposition et tentez quelques recherches dans votre index.</strong></p>

			</div>
		</div>
	</div>
	<footer class="footer">
		<div class="container">
			<div class="span8">
				<dl>
					<dt>Ressources</dt>
					<dd>
						<ul>
							<li><a href="http://nltk.googlecode.com/svn/trunk/doc/book/ch05.html">Chapitre du livre NLTK sur l'étiquetage</a></li>
							<li>Billet de streamhacker sur le <a href="http://streamhacker.com/2011/03/21/training-part-speech-taggers-nltk-trainer/">pos tagging</a> et le <a href="http://streamhacker.com/2008/12/29/how-to-train-a-nltk-chunker/">chunking</a></li>
							<li><a href="https://gforge.inria.fr/frs/download.php/31734/sequoia-corpus-v4.0.tgz">Séquoia : un corpus arboré pour le français<a/></li>
						</ul>
					</dd>
					<dt>Corpus, code, ...</dt>
					<dd><a href="https://github.com/grdscarabe/nlp-lessons">https://github.com/grdscarabe/nlp-lessons</a></dd>
					<dt>Lectures</dt>
					<dd><a href="http://www.fabienpoulard.info">Blog de l'auteur</a></dd>
				</dl>
			</div>
			<div class="span3">
				<address>
					<strong>Fabien Poulard</strong><br>
					Dictanova SAS<br>
					2, chemin de la Houssinière<br/>
					44300 Nantes</br>
					<a href="mailto:fpoulard@dictanova.com">fpoulard@dictanova.com</a>
				</address>
			</div>
		</div>
	</footer>
	<!-- JS -->
	<script src="http://code.jquery.com/jquery-latest.js"></script>
	<script src="../commons/js/bootstrap.min.js"></script>
</body>
</html>


